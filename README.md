# CPPGrad: A C++ Neural Network Framework with Automatic Differentiation

A modern, educational C++ implementation of a neural network framework featuring automatic differentiation, computational graphs, and a complete training pipeline. Built from scratch to demonstrate deep learning fundamentals while maintaining production-quality code structure.

## Features

- **Automatic Differentiation**: Full backward pass implementation with computational graph tracking
- **Tensor Operations**: Multi-dimensional arrays with gradient computation support
- **Neural Network Layers**: Linear layers, ReLU activations, and sequential model containers
- **Optimization**: Adam optimizer with momentum and adaptive learning rates
- **Data Pipeline**: CSV loading, preprocessing, and normalization utilities
- **Memory Management**: Intelligent computational graph lifecycle management
- **Training Loop**: Complete training pipeline with early stopping and monitoring

## Architecture

### Core Components

- **`Tensor`**: Multi-dimensional arrays with automatic gradient tracking
- **`Op`**: Base class for all computational operations
- **`Module`**: Abstract interface for neural network layers
- **`Graph`**: Computational graph memory manager
- **`Sequential`**: Container for chaining neural network modules

### Neural Network Operations

- **Matrix Multiplication**: Core linear transformations
- **Element-wise Operations**: Addition, subtraction, multiplication, division
- **Activation Functions**: ReLU implementation
- **Loss Functions**: Mean Squared Error (MSE)
- **Reduction Operations**: Mean computation

### Training Infrastructure

- **Adam Optimizer**: Adaptive moment estimation with momentum
- **Data Loading**: Robust CSV parsing and validation
- **Normalization**: Min-max scaling for stable training
- **Monitoring**: Gradient flow analysis and parameter tracking
- **Early Stopping**: Prevents overfitting with patience-based stopping

## Project Structure

cppgrad/
â”œâ”€â”€ CMakeLists.txt              # Build configuration
â”œâ”€â”€ main.cpp                    # Training script and main entry point
â”œâ”€â”€ tensor.hpp                  # Core tensor class definition
â”œâ”€â”€ tensor.cpp                  # Tensor implementation
â”œâ”€â”€ op.hpp                      # Base operation class
â”œâ”€â”€ graph.hpp                   # Computational graph manager
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ module.hpp             # Base neural network module
â”‚   â””â”€â”€ module.cpp             # Module implementation
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ sequential.hpp         # Sequential model container
â”‚   â””â”€â”€ sequential.cpp         # Sequential implementation
â”œâ”€â”€ ops/                       # Neural network operations
â”‚   â”œâ”€â”€ add.cpp/hpp           # Addition operation
â”‚   â”œâ”€â”€ sub.cpp/hpp           # Subtraction operation
â”‚   â”œâ”€â”€ mul.cpp/hpp           # Multiplication operation
â”‚   â”œâ”€â”€ div.cpp/hpp           # Division operation
â”‚   â”œâ”€â”€ matmul.cpp/hpp        # Matrix multiplication
â”‚   â”œâ”€â”€ mse.cpp/hpp           # Mean squared error loss
â”‚   â”œâ”€â”€ mean.hpp              # Mean reduction
â”‚   â”œâ”€â”€ pow.cpp/hpp           # Power operation
â”‚   â””â”€â”€ linear_op.cpp/hpp     # Linear layer operation
â”œâ”€â”€ optimizer/
â”‚   â”œâ”€â”€ adam.cpp              # Adam optimizer implementation
â”‚   â””â”€â”€ adam.hpp              # Adam optimizer interface
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ csv_loader.cpp        # CSV data loading utilities
â”‚   â”œâ”€â”€ csv_loader.hpp        # Data loading interface
â”‚   â””â”€â”€ housing_clean.csv     # California housing dataset
â”œâ”€â”€ linear.cpp                 # Linear layer implementation
â”œâ”€â”€ linear.hpp                 # Linear layer interface
â”œâ”€â”€ relu.cpp                   # ReLU activation implementation
â”œâ”€â”€ relu.hpp                   # ReLU activation interface
â””â”€â”€ tensor_ops.hpp             # Tensor operation utilities


## Building and Running

### Prerequisites

- **C++17** compatible compiler (GCC 7+, Clang 5+, MSVC 2017+)
- **CMake 3.14** or higher
- **Make** or **Ninja** build system

### Build Instructions

```bash
# Clone the repository
git clone https://github.com/nicbov/Deep-Learning-Framework-CPP.git
cd cppgrad

# Create build directory
mkdir build && cd build

# Configure with CMake
cmake ..

# Build the project
make -j$(nproc)

# Run the training script
./cppgrad
```

### Alternative Build Methods

```bash
# Using Ninja (faster builds)
cmake -G Ninja ..
ninja

# Using specific compiler
cmake -DCMAKE_CXX_COMPILER=g++-9 ..
make
```

## Usage Example

The main training script demonstrates a complete neural network training pipeline:

```cpp
#include "model/sequential.hpp"
#include "optimizer/adam.hpp"
#include "data/csv_loader.hpp"

int main() {
    // Load and preprocess California housing data
    auto data = load_csv("data/housing_clean.csv");
    
    // Build neural network: 9 -> 16 -> 8 -> 4 -> 1
    auto model = std::make_shared<Sequential>();
    model->add_module(std::make_shared<Linear>(9, 16));
    model->add_module(std::make_shared<ReLU>());
    model->add_module(std::make_shared<Linear>(16, 8));
    model->add_module(std::make_shared<ReLU>());
    model->add_module(std::make_shared<Linear>(8, 4));
    model->add_module(std::make_shared<ReLU>());
    model->add_module(std::make_shared<Linear>(4, 1));
    
    // Train with Adam optimizer
    Adam optimizer(0.01f);
    
    // Training loop with early stopping
    for (int epoch = 0; epoch < 200; ++epoch) {
        model->zero_grad();
        auto output = model->forward(input);
        auto loss = mse_loss(output, target);
        loss->backward();
        optimizer.step(model->parameters());
    }
    
    return 0;
}
```

## Dataset

The project includes the **California Housing Dataset** (`housing_clean.csv`) with the following features:

- **Geographic**: longitude, latitude
- **Property**: housing_median_age, total_rooms, total_bedrooms
- **Demographic**: population, households, median_income
- **Categorical**: ocean_proximity (encoded as numeric)
- **Target**: median_house_value (regression target)

**Dataset Statistics**:
- **Samples**: 20,434 housing records
- **Features**: 9 input dimensions
- **Target Range**: $14,999 - $500,001
- **Preprocessing**: Cleaned and normalized for training

## ðŸ”¬ Technical Details

### Automatic Differentiation

The framework implements reverse-mode automatic differentiation:

1. **Forward Pass**: Creates computational graph with operation nodes
2. **Gradient Computation**: Each operation knows how to compute gradients w.r.t. inputs
3. **Backward Pass**: Propagates gradients through the graph using chain rule
4. **Memory Management**: Graph manager prevents premature tensor destruction

### Neural Network Architecture

- **Input Layer**: 9 features (housing characteristics)
- **Hidden Layer 1**: 16 neurons with ReLU activation
- **Hidden Layer 2**: 8 neurons with ReLU activation  
- **Hidden Layer 3**: 4 neurons with ReLU activation
- **Output Layer**: 1 neuron (house price prediction)

### Training Configuration

- **Optimizer**: Adam with learning rate 0.01
- **Loss Function**: Mean Squared Error (MSE)
- **Normalization**: Min-max scaling to [0,1] range
- **Early Stopping**: 20 epochs patience
- **Gradient Clipping**: Maximum norm 1.0
- **Maximum Epochs**: 200

## Testing and Validation

The project includes comprehensive testing and debugging features:

- **Gradient Monitoring**: Tracks gradient flow through the network
- **Parameter Tracking**: Monitors weight changes during training
- **Prediction Validation**: Shows predictions vs. targets every 10 epochs
- **Memory Leak Prevention**: Computational graph cleanup after each step
- **Numerical Stability**: NaN/Inf detection and early stopping

